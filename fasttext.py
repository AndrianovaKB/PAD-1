# -*- coding: utf-8 -*-
"""Fasttext.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p8_LcS_75pkHyFL4E6PEULlnmR9y37il
"""

import gensim
from gensim.models import FastText

pip install gensim

from sklearn.datasets import fetch_20newsgroups
import pandas as pd
categories = ['rec.motorcycles','soc.religion.christian','rec.sport.hockey','rec.sport.baseball','sci.med']
remove = ('headers', 'footers')
twenty_groups = fetch_20newsgroups(subset='all', shuffle=True, random_state=42, categories = categories, remove = remove )
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(twenty_groups.data,
                                                    twenty_groups.target,
                                                    test_size=0.25,
                                                    random_state=33)
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import string
# Загрузка стоп-слов
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
# Инициализация лемматизатора
lemmatizer = WordNetLemmatizer()
# Функция для предобработки текста
def preprocess_text(text):
    # Удаление знаков пунктуации и цифр
    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))
    # Токенизация текста
    tokens = word_tokenize(text.lower())
    # Лемматизация и удаление стоп-слов
    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]
    return tokens
import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer
# Предобработка каждого документа в наборе данных
processed_data = []
for document in twenty_groups.data:
    processed_document = preprocess_text(document)
    processed_data.append(processed_document)

# Преобразование массива массивов в numpy-массив
#processed_data = np.array(processed_data)

# Вывод обработанных данных
print(processed_data[:5])
from sklearn.model_selection import train_test_split
x_train1, x_test1, y_train1, y_test1 = train_test_split(processed_data,
                                                    twenty_groups.target,
                                                    test_size=0.25,
                                                    random_state=33)

from time import time

fttt_model=FastText(vector_size=300, window=2, min_count=1, sample=6e-5, alpha=0.02, min_alpha=0.0007, negative=20)

t=time()
fttt_model.build_vocab(x_train1)
print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))

fttt_model.train(x_train1, total_examples=fttt_model.corpus_count, epochs=10000, report_delay=1)
print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))

import logging
logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

import gensim.downloader as api

model = api.load("glove-wiki-gigaword-50")

def vectorize(list_of_docs, model):
  import numpy as np
  """Generate vectors for list of documents using a Word Embedding
  Args:
  list_of_docs: List of documents
  model: Gensim's Word Embedding
  Returns:
  List of document vectors
  """
  features = []
  for tokens in list_of_docs:
    zero_vector = np.zeros(model.vector_size)
    vectors = []
    for token in tokens:
      if token in model:
        try:
          vectors.append(model[token])
        except KeyError:
          continue
    if vectors:
      vectors = np.asarray(vectors)
      avg_vec = vectors.mean(axis=0)
      features.append(avg_vec)
    else:
      features.append(zero_vector)
  return features

vectorized_docs_train = vectorize(x_train1, model = model)
vectorized_docs_test = vectorize(x_test1, model = model)

from sklearn.svm import SVC
from sklearn.metrics import classification_report
svm = SVC()
svm.fit(vectorized_docs_train, y_train1)
y_pred = svm.predict(vectorized_docs_test)
print("Более подробные показатели оценки: \n", classification_report(y_pred, y_test1))

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report
from sklearn.naive_bayes import MultinomialNB
scaler = MinMaxScaler()
X_train = scaler.fit_transform(vectorized_docs_train)
X_test = scaler.transform(vectorized_docs_test)
mnb_tfid_stop = MultinomialNB()
mnb_tfid_stop.fit(X_train, y_train1)      # Учить
mnb_tfid_stop_y_predict = mnb_tfid_stop.predict(X_test)        # Прогноз
print("Точность модели обучения функции:", mnb_tfid_stop.score(X_test, y_test1))
print("Более подробные показатели оценки: \n", classification_report(mnb_tfid_stop_y_predict, y_test1))

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MinMaxScaler
import numpy as np
forest = RandomForestClassifier()
from sklearn.naive_bayes import MultinomialNB
scaler = MinMaxScaler()
X_train = scaler.fit_transform(vectorized_docs_train)
X_test = scaler.transform(vectorized_docs_test)
forest.fit(X_train, y_train1)
y_pred_test1 = forest.predict(X_test)
print(classification_report(y_pred_test1, y_test1))

from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train = scaler.fit_transform(vectorized_docs_train)
X_test = scaler.transform(vectorized_docs_test)
clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train1)
# Прогнозирование классов для тестового набора данных
y_pred1 = clf.predict(X_test)
print(classification_report(y_pred1, y_test1))